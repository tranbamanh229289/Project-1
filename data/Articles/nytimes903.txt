CHICAGO — When Eric L. Loomis was sentenced for eluding the police in La Crosse, Wis., the judge told him he presented a “high risk” to the community and handed down a six-year prison term.
The judge said he had arrived at his sentencing decision in part because of Mr. Loomis's rating on the Compas assessment, a secret algorithm used in the Wisconsin justice system to calculate the likelihood that someone will commit another crime.
Mr. Loomis has challenged the judge's reliance on the Compas score, and the Wisconsin Supreme Court, which heard arguments on his appeal in April, could rule in the coming days or weeks. Mr. Loomis's appeal centers on the criteria used by the Compas algorithm, which is proprietary and as a result is protected, and on the differences in its application for men and women.
The debate in Wisconsin highlights a broader national discussion about how law enforcement officials use predictive data — including deciding which streets to patrol, identifying people at risk of being shot and calculating the likelihood of recidivism.
Mr. Loomis was arrested in February 2013 and was accused of driving a car that had been used in a shooting. He pleaded guilty to eluding an officer and no contest to operating a vehicle without the owner's consent.
Mr. Loomis, 34, is a registered sex offender, stemming from a past conviction for third-degree sexual assault.
Before his sentencing for his 2013 arrest, Mr. Loomis received a score on the Compas scale that suggested he was at a high risk of committing another crime. He is now serving his six-year sentence, with a possible release in 2019.
After Mr. Loomis appealed his sentence, an appellate court referred the case to the Wisconsin Supreme Court. The Wisconsin attorney general's office has defended the use of the Compas system in assessing risk, saying in court filings that it “has a role at sentencing” and is “individualized to each defendant.”
Compas is an algorithm developed by a private company, Northpointe Inc., that calculates the likelihood of someone committing another crime and suggests what kind of supervision a defendant should receive in prison. The results come from a survey of the defendant and information about his or her past conduct. Compas assessments are a data-driven complement to the written presentencing reports long compiled by law enforcement agencies.
Company officials say the algorithm's results are backed by research, but they are tight-lipped about its details. They do acknowledge that men and women receive different assessments, as do juveniles, but the factors considered and the weight given to each are kept secret.
“The key to our product is the algorithms, and they're proprietary,” said Jeffrey Harmon, Northpointe's general manager. “We've created them, and we don't release them because it's certainly a core piece of our business. It's not about looking at the algorithms. It's about looking at the outcomes.”
That secrecy is at the heart of Mr. Loomis's lawsuit. His lawyer, Michael D. Rosenberg, who declined to be interviewed because of the pending appeal, argued that Mr. Loomis should be able to review the algorithm and make arguments about its validity as part of his defense. He also challenges the use of different scales for each sex.
The Compas system, Mr. Rosenberg wrote in his brief, “is full of holes and violates the requirement that a sentence be individualized.”
Increasingly, yes. Many states use algorithms, often called “risk assessments,” as part of the sentencing process, though the formulas and uses vary from place to place.
Utah employs a method that weighs what state officials call the “Big Four” factors that they say most correlate with recidivism: history of behavior that harms others, antisocial personality patterns, attitudes and beliefs that favor crime, and “association with pro-criminal peers.”
Virginia has used algorithms in its sentencing process for more than a decade.
But legal tests have been few. A prison sentence involving Compas was previously appealed in Wisconsin and upheld. In Indiana, the State Supreme Court ruled in 2010 that judges could consider risk assessments as one of several factors in handing down sentences.
Many law enforcement agencies use software to predict potential crime hot spots, and the police in Kansas City, Mo., and other places have used data to identify potential criminals and to try to intervene.
“This is just the next innovation of crime analysis — trying to get ahead of the problem, trying to predict where problems occur before they actually occur,” said Eric L. Piza, an assistant professor at the John Jay College of Criminal Justice who studies predictive policing.
In Chicago, where there has been a sharp rise in violent crime this year, the police have used an algorithm to compile a list of people most likely to shoot or be shot. Over Memorial Day weekend, when 64 people were shot in Chicago, the police said 50 of the victims were on that list.
Algorithms like Compas are also used commonly in prison systems to identify the types of supervision inmates might need, the most appropriate type of incarceration and the risk of committing another crime if released on bail or parole.
In Wisconsin, Tristan Cook, a spokesman for the state's Department of Corrections, said Compas scores were used to help with inmate classification and release planning and were also available to sentencing judges upon request.
There is general support for identifying people who pose little threat to the community, so they could be taken out of America's crowded prison systems. Such algorithms are also seen as a way to dispense justice in a more efficient way that relies more on numerical evidence than on personal judgments.
In Pennsylvania, the Legislature directed the state's sentencing commission in 2010 to develop a set of algorithms that could be used to inform sentencing judges about a defendant's risk of being arrested again. After six years of work, the proposed algorithms are set to be tested this summer in four counties.
Mark H. Bergstrom, the executive director of the Pennsylvania Commission on Sentencing, said he hoped the algorithms would help judges differentiate typical defendants, who might be well served with a sentence in the recommended range, from those the algorithms identify as having a particularly high or low risk of committing another crime. In those outlier cases, Mr. Bergstrom said he would recommend that a judge order a fuller presentencing report to help make a more individualized decision.
As Mr. Bergstrom said about the system being developed in Pennsylvania, “we're talking about group characteristics,” and judges have to be educated to use algorithm results in a “meaningful way, and not a destructive way, with the individual before them.”
Because they are being developed by a public agency, the proposed Pennsylvania algorithms are available for the public to analyze. The models include gender and past arrests, among other factors. As is common with these algorithm models, the Pennsylvania sentencing commission excluded race.
Ezekiel Edwards, the director of the Criminal Law Reform Project at the American Civil Liberties Union, said a recurring fear with predictive algorithms was that they could accentuate inequalities in other areas of the justice system.
He also said data from the criminal justice system was often unreliable and that it could call into question results from those algorithms.
Mr. Edwards urged caution in testing the results and eliminating any prejudices in the algorithms.
“I think we are kind of rushing into the world of tomorrow with big-data risk assessment,” he said, “without properly vetting, studying and ensuring that we minimize a lot of these potential biases in the data.”

